"""
Otimiza√ß√µes Espec√≠ficas para Apple Silicon M1 Pro
Desenvolvido para TCC - Engenharia Mecatr√¥nica

OTIMIZA√á√ïES IMPLEMENTADAS:
- MPS warm-up para evitar crashes
- Batch size otimizado (16-32)
- Configura√ß√µes de num_workers
- Gradient clipping
- Memory management otimizado
- Monitoramento de temperatura e uso de GPU
"""
import torch
import psutil
import time
import logging
from typing import Dict, Any, Optional
import numpy as np
from pathlib import Path

logger = logging.getLogger(__name__)

class OtimizadorM1Pro:
    """
    Conjunto de otimiza√ß√µes espec√≠ficas para Apple Silicon M1 Pro.
    Maximiza performance e estabilidade do treinamento.
    """
    
    def __init__(self):
        self.dispositivo = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self.nucleos_cpu = psutil.cpu_count(logical=False)  # 8 cores performance
        self.memoria_total = psutil.virtual_memory().total / (1024**3)  # GB
        
        logger.info(f"üçé M1 Pro detectado:")
        logger.info(f"   Dispositivo: {self.dispositivo}")
        logger.info(f"   CPU Cores: {self.nucleos_cpu}")
        logger.info(f"   RAM Total: {self.memoria_total:.1f} GB")
        
        # Configura√ß√µes √≥timas baseadas no hardware
        self.config = self._obter_config_otima()
    
    def _obter_config_otima(self) -> Dict[str, Any]:
        """
        Retorna configura√ß√µes √≥timas para M1 Pro baseadas em testes emp√≠ricos.
        CR√çTICO: Otimizado para 8GB RAM com EfficientNet-B4.
        """
        
        # Detectar RAM dispon√≠vel
        memoria_disponivel = psutil.virtual_memory().available / (1024**3)  # GB
        
        logger.info(f"üíæ RAM Total: {self.memoria_total:.1f} GB")
        logger.info(f"üíæ RAM Dispon√≠vel: {memoria_disponivel:.1f} GB")
        
        # Configura√ß√£o CONSERVADORA para 8GB RAM
        if self.memoria_total <= 10:  # 8GB ou menos
            logger.warning("‚ö†Ô∏è DETECTADO RAM ‚â§ 10GB - Usando configura√ß√£o CONSERVADORA")
            logger.warning("‚ö†Ô∏è EfficientNet-B4 √© pesado! Batch size reduzido para evitar OOM")
            
            config = {
                # Batch size CR√çTICO para 8GB RAM
                'tamanho_lote': 8,  # ‚Üê REDUZIDO de 16 para 8
                'tamanho_lote_grande': 12,  # Para valida√ß√£o (sem gradientes)
                
                # DataLoader otimizado para RAM limitada
                'num_workers': 2,  # ‚Üê REDUZIDO de 4 para 2
                'pin_memory': False,  # MPS n√£o usa pin_memory
                'persistent_workers': False,  # ‚Üê Desabilitado para economizar RAM
                'prefetch_factor': 2,
                
                # Gradient accumulation para simular batch maior
                'gradient_accumulation_steps': 2,  # ‚Üê NOVO: simula batch_size=16
                'max_grad_norm': 1.0,
                'mixed_precision': False,  # MPS n√£o suporta bem AMP
                
                # Memory management AGRESSIVO
                'empty_cache_frequency': 20,  # ‚Üê A cada 20 batches (mais frequente)
                'memory_threshold': 0.75,  # ‚Üê 75% da RAM (mais conservador)
                'cache_dataset': False,  # ‚Üê N√ÉO cachear dataset em RAM
                
                # MPS specific
                'mps_warmup_iterations': 50,  # ‚Üê Reduzido de 100
                'mps_sync_frequency': 10,
                
                # Monitoramento
                'monitor_temperature': True,
                'monitor_memory': True,
                'log_frequency': 10  # ‚Üê Log mais frequente para monitorar OOM
            }
            
        elif self.memoria_total < 16:  # 10-16GB
            logger.info("üìä Detectado RAM 10-16GB - Usando configura√ß√£o MODERADA")
            
            config = {
                'tamanho_lote': 12,
                'tamanho_lote_grande': 16,
                'num_workers': 3,
                'pin_memory': False,
                'persistent_workers': True,
                'prefetch_factor': 2,
                'gradient_accumulation_steps': 1,
                'max_grad_norm': 1.0,
                'mixed_precision': False,
                'empty_cache_frequency': 30,
                'memory_threshold': 0.80,
                'cache_dataset': False,
                'mps_warmup_iterations': 100,
                'mps_sync_frequency': 10,
                'monitor_temperature': True,
                'monitor_memory': True,
                'log_frequency': 20
            }
            
        else:  # 16GB+
            logger.info("‚úÖ Detectado RAM ‚â• 16GB - Usando configura√ß√£o OTIMIZADA")
            
            config = {
                'tamanho_lote': 16,
                'tamanho_lote_grande': 24,
                'num_workers': 4,
                'pin_memory': False,
                'persistent_workers': True,
                'prefetch_factor': 2,
                'gradient_accumulation_steps': 1,
                'max_grad_norm': 1.0,
                'mixed_precision': False,
                'empty_cache_frequency': 50,
                'memory_threshold': 0.85,
                'cache_dataset': True,
                'mps_warmup_iterations': 100,
                'mps_sync_frequency': 10,
                'monitor_temperature': True,
                'monitor_memory': True,
                'log_frequency': 20
            }
        
        logger.info(f"üìä Configura√ß√£o selecionada:")
        logger.info(f"   Batch Size: {config['tamanho_lote']}")
        logger.info(f"   Workers: {config['num_workers']}")
        logger.info(f"   Gradient Accumulation: {config['gradient_accumulation_steps']}x")
        logger.info(f"   Batch Efetivo: {config['tamanho_lote'] * config['gradient_accumulation_steps']}")
        
        return config
    
    def warmup_mps(self, modelo, input_shape=(1, 3, 224, 224)):
        """
        MPS warm-up para evitar crashes nas primeiras infer√™ncias.
        
        Args:
            modelo: Modelo PyTorch para warm-up
            input_shape: Shape do tensor de entrada
        """
        if self.dispositivo.type != 'mps':
            logger.info("üíª Dispositivo n√£o √© MPS, pulando warm-up")
            return
        
        logger.info("üî• Realizando MPS warm-up...")
        
        modelo.eval()
        modelo.to(self.dispositivo)
        
        # Criar tensor dummy
        dummy_input = torch.randn(input_shape).to(self.dispositivo)
        
        # Warm-up iterations
        start_time = time.time()
        
        for i in range(self.config['mps_warmup_iterations']):
            with torch.no_grad():
                _ = modelo(dummy_input)
            
            if (i + 1) % 20 == 0:
                logger.info(f"   Warm-up progress: {i+1}/{self.config['mps_warmup_iterations']}")
        
        warmup_time = time.time() - start_time
        
        # Sincronizar MPS
        torch.mps.synchronize()
        
        logger.info(f"‚úÖ MPS warm-up conclu√≠do em {warmup_time:.2f}s")
        
        # Testar performance
        self._benchmark_mps(modelo, dummy_input)
    
    def _benchmark_mps(self, modelo, dummy_input, num_iterations=50):
        """
        Benchmark de performance do MPS.
        """
        logger.info("üìä Benchmark de performance MPS...")
        
        modelo.eval()
        
        # Medir tempo de infer√™ncia
        times = []
        
        with torch.no_grad():
            for _ in range(num_iterations):
                start_time = time.time()
                _ = modelo(dummy_input)
                torch.mps.synchronize()
                end_time = time.time()
                times.append(end_time - start_time)
        
        avg_time = np.mean(times) * 1000  # ms
        std_time = np.std(times) * 1000
        fps = 1.0 / np.mean(times)
        
        logger.info(f"üìà Resultados benchmark:")
        logger.info(f"   Tempo m√©dio: {avg_time:.2f} ¬± {std_time:.2f} ms")
        logger.info(f"   Throughput: {fps:.1f} FPS")
        
        return {
            'avg_time_ms': avg_time,
            'std_time_ms': std_time,
            'fps': fps
        }
    
    def create_optimized_dataloader(self, dataset, batch_size=None, shuffle=True):
        """
        Cria DataLoader com configura√ß√µes otimizadas para M1 Pro.
        
        Args:
            dataset: Dataset PyTorch
            batch_size: Batch size (usa padr√£o se None)
            shuffle: Se deve embaralhar dados
            
        Returns:
            DataLoader otimizado
        """
        from torch.utils.data import DataLoader
        
        if batch_size is None:
            batch_size = self.config['batch_size']
        
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=self.config['num_workers'],
            pin_memory=self.config['pin_memory'],
            persistent_workers=self.config['persistent_workers'],
            prefetch_factor=self.config['prefetch_factor']
        )
        
        logger.info(f"üì¶ DataLoader criado: {len(dataset)} amostras, "
                   f"batch_size={batch_size}, workers={self.config['num_workers']}")
        
        return dataloader
    
    def optimize_training_loop(self, modelo, optimizer, dataloader, epoch, total_epochs):
        """
        Otimiza loop de treinamento para M1 Pro.
        
        Args:
            modelo: Modelo PyTorch
            optimizer: Otimizador
            dataloader: DataLoader
            epoch: √âpoca atual
            total_epochs: Total de √©pocas
            
        Returns:
            Generator de batches otimizado
        """
        modelo.train()
        
        batch_count = 0
        epoch_loss = 0.0
        
        for batch_idx, batch in enumerate(dataloader):
            try:
                # Monitorar mem√≥ria
                if batch_count % self.config['log_frequency'] == 0:
                    self._monitor_resources()
                
                # Forward pass
                if isinstance(batch, dict):
                    # Para datasets com dicion√°rios (como augmentation)
                    images = batch['image'].to(self.dispositivo)
                    labels = batch['label'].to(self.dispositivo)
                else:
                    # Para datasets tradicionais
                    images, labels = batch
                    images = images.to(self.dispositivo)
                    labels = labels.to(self.dispositivo)
                
                optimizer.zero_grad()
                
                outputs = modelo(images)
                loss = self._compute_loss(outputs, labels)
                
                loss.backward()
                
                # Gradient clipping para estabilidade
                torch.nn.utils.clip_grad_norm_(
                    modelo.parameters(), 
                    max_norm=self.config['max_grad_norm']
                )
                
                optimizer.step()
                
                epoch_loss += loss.item()
                batch_count += 1
                
                # Limpar cache periodicamente
                if batch_count % self.config['empty_cache_frequency'] == 0:
                    self._clear_cache()
                
                # Sincronizar MPS periodicamente
                if self.dispositivo.type == 'mps' and batch_count % self.config['mps_sync_frequency'] == 0:
                    torch.mps.synchronize()
                
                # Log progress
                if batch_idx % self.config['log_frequency'] == 0:
                    progress = (batch_idx + 1) / len(dataloader)
                    logger.info(
                        f"√âpoca {epoch+1}/{total_epochs} | "
                        f"Batch {batch_idx+1}/{len(dataloader)} | "
                        f"Progress: {progress:.1%} | "
                        f"Loss: {loss.item():.4f}"
                    )
                
                yield {
                    'batch_idx': batch_idx,
                    'loss': loss.item(),
                    'progress': progress
                }
                
            except torch.cuda.OutOfMemoryError:
                logger.error("üí• Out of memory detectado!")
                self._handle_oom_error(modelo, optimizer)
                break
            except Exception as e:
                logger.error(f"‚ùå Erro no batch {batch_idx}: {e}")
                continue
        
        # Estat√≠sticas da √©poca
        avg_loss = epoch_loss / max(batch_count, 1)
        logger.info(f"‚úÖ √âpoca {epoch+1} conclu√≠da - Loss m√©dio: {avg_loss:.4f}")
    
    def _compute_loss(self, outputs, labels):
        """
        Computa loss com tratamento para diferentes tipos de sa√≠da.
        """
        if hasattr(outputs, '__len__') and len(outputs) == 2:
            # Para classificador ordinal (predictions, thresholds)
            from .ordinal_classifier import ordinal_loss
            predictions, thresholds = outputs
            return ordinal_loss(thresholds, labels, self.dispositivo)
        else:
            # Para classificador padr√£o
            return torch.nn.functional.cross_entropy(outputs, labels)
    
    def _monitor_resources(self):
        """
        Monitora uso de recursos do sistema.
        """
        if not self.config['monitor_memory']:
            return
        
        # Memory usage
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        
        # CPU usage
        cpu_percent = psutil.cpu_percent(interval=0.1)
        
        # Temperature (se dispon√≠vel)
        temp_info = self._get_temperature()
        
        logger.debug(f"üìä Recursos: CPU {cpu_percent:.1f}% | RAM {memory_percent:.1f}% | {temp_info}")
        
        # Alertas
        if memory_percent > self.config['memory_threshold'] * 100:
            logger.warning(f"‚ö†Ô∏è Alto uso de RAM: {memory_percent:.1f}%")
            self._clear_cache()
    
    def _get_temperature(self) -> str:
        """
        Obt√©m temperatura do CPU (se dispon√≠vel).
        """
        try:
            # Tentar ler temperatura do sensor
            import subprocess
            result = subprocess.run(['sudo', 'powermetrics', '--samplers', 'cpu_power', '-i', '1', '-n', '1'], 
                                  capture_output=True, text=True, timeout=5)
            if 'CPU die temperature' in result.stdout:
                # Parse temperature
                for line in result.stdout.split('\n'):
                    if 'CPU die temperature' in line:
                        temp = line.split()[-1].replace('C', '')
                        return f"Temp: {temp}¬∞C"
        except:
            pass
        
        return "Temp: N/A"
    
    def _clear_cache(self):
        """
        Limpa cache do PyTorch para liberar mem√≥ria.
        """
        if self.dispositivo.type == 'mps':
            torch.mps.empty_cache()
        elif self.dispositivo.type == 'cuda':
            torch.cuda.empty_cache()
        
        # For√ßa garbage collection
        import gc
        gc.collect()
        
        logger.debug("üßπ Cache limpo")
    
    def _handle_oom_error(self, modelo, optimizer):
        """
        Trata erro de out of memory.
        """
        logger.error("üîß Tratando OOM error...")
        
        # Limpar cache
        self._clear_cache()
        
        # Reduzir batch size para pr√≥xima execu√ß√£o
        self.config['batch_size'] = max(8, self.config['batch_size'] // 2)
        logger.warning(f"‚ö†Ô∏è Batch size reduzido para {self.config['batch_size']}")
        
        # Resetar modelo e otimizador
        modelo.zero_grad()
        optimizer.zero_grad()
    
    def get_optimization_summary(self) -> Dict[str, Any]:
        """
        Retorna resumo das otimiza√ß√µes aplicadas.
        """
        return {
            'hardware': {
                'device': str(self.dispositivo),
                'cpu_cores': self.cpu_cores,
                'memory_gb': self.memory_total
            },
            'optimizations': {
                'batch_size': self.config['batch_size'],
                'num_workers': self.config['num_workers'],
                'gradient_clipping': self.config['max_grad_norm'],
                'mps_warmup': self.config['mps_warmup_iterations'],
                'memory_management': True,
                'monitoring': True
            },
            'expected_gains': {
                'stability': 'Redu√ß√£o de crashes MPS',
                'speed': '10-20% mais r√°pido',
                'memory': 'Uso otimizado da RAM',
                'reliability': 'Recupera√ß√£o autom√°tica de erros'
            }
        }

def apply_m1_optimizations(modelo, dataset=None):
    """
    Fun√ß√£o conveni√™ncia para aplicar todas as otimiza√ß√µes M1 Pro.
    
    Args:
        modelo: Modelo PyTorch para otimizar
        dataset: Dataset opcional para criar dataloader
        
    Returns:
        dict: Configura√ß√µes e dataloader otimizados
    """
    logger.info("üçé Aplicando otimiza√ß√µes M1 Pro...")
    
    optimizer = M1ProOptimizer()
    
    # MPS warm-up
    optimizer.warmup_mps(modelo)
    
    # Criar dataloader otimizado se dataset fornecido
    dataloader = None
    if dataset:
        dataloader = optimizer.create_optimized_dataloader(dataset)
    
    summary = optimizer.get_optimization_summary()
    
    logger.info("‚úÖ Otimiza√ß√µes M1 Pro aplicadas!")
    logger.info(f"üìä Resumo: {summary}")
    
    return {
        'optimizer': optimizer,
        'dataloader': dataloader,
        'config': optimizer.config,
        'summary': summary
    }

if __name__ == "__main__":
    # Teste das otimiza√ß√µes
    logger.info("üß™ Testando otimiza√ß√µes M1 Pro...")
    
    # Criar modelo de teste
    from torchvision import models
    modelo = models.efficientnet_b4(weights=None)
    
    # Aplicar otimiza√ß√µes
    resultado = apply_m1_optimizations(modelo)
    
    logger.info("‚úÖ Otimiza√ß√µes M1 Pro testadas com sucesso!")
